# ClarifAI Central Configuration File
# This file contains all configurable parameters for the ClarifAI system
# Following the architecture principle: Never hardcode values

# Version and system info
version: "0.1.0"
config_version: 1

# Database configurations
databases:
  postgres:
    # Connection settings (can be overridden by environment variables)
    host: "postgres"  # POSTGRES_HOST
    port: 5432        # POSTGRES_PORT
    database: "clarifai"  # POSTGRES_DB
    # User and password come from environment variables only
    # POSTGRES_USER, POSTGRES_PASSWORD
    
  neo4j:
    # Connection settings (can be overridden by environment variables)
    host: "neo4j"     # NEO4J_HOST
    port: 7687        # NEO4J_BOLT_PORT
    # User and password come from environment variables only
    # NEO4J_USER, NEO4J_PASSWORD

# Vault and path configurations
paths:
  vault: "/vault"
  tier1: "conversations"    # Relative to vault path
  tier2: "summaries"       # Relative to vault path
  tier3: "concepts"        # Relative to vault path
  settings: "/settings"

# Embedding configuration
embedding:
  # Models configuration
  models:
    default: "sentence-transformers/all-MiniLM-L6-v2"
    # Alternative models can be configured here
    # large: "sentence-transformers/all-mpnet-base-v2"
    # multilingual: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
  
  # Embedding settings
  device: "auto"  # "auto", "cpu", "cuda", "mps"
  batch_size: 32
  
  # PGVector settings
  pgvector:
    collection_name: "utterances"
    embed_dim: 384  # Dimension for all-MiniLM-L6-v2
    # Index settings for pgvector
    index_type: "ivfflat"
    index_lists: 100  # Number of lists for IVFFlat index
  
  # Chunking configuration
  chunking:
    # SentenceSplitter parameters following on-sentence_splitting.md
    chunk_size: 300
    chunk_overlap: 30
    keep_separator: true
    
    # Post-processing rules for coherent chunks
    merge_colon_endings: true    # Merge "text:" + "continuation" 
    merge_short_prefixes: true   # Merge fragments < 5 tokens
    min_chunk_tokens: 5          # Minimum tokens per chunk

# LLM configuration
llm:
  # Default models for different tasks
  models:
    default: "gpt-3.5-turbo"
    fallback_plugin: "gpt-3.5-turbo"
    conversation_extraction: "gpt-3.5-turbo"
  
  # Model parameters
  temperature: 0.1
  max_tokens: 1000
  timeout: 30

# Model configuration (following design_config_panel.md structure)
model:
  claimify:
    default: "gpt-3.5-turbo"
    selection: null      # Uses default if not specified
    disambiguation: null # Uses default if not specified  
    decomposition: null  # Uses default if not specified
  fallback_plugin: "gpt-3.5-turbo"

# Threshold configuration (following design_config_panel.md structure)
threshold:
  concept_merge: 0.90
  claim_link_strength: 0.60

# Window configuration (following design_config_panel.md structure)
window:
  claimify:
    p: 3  # Previous sentences
    f: 1  # Following sentences

# Processing configuration for Claimify
processing:
  claimify:
    max_retries: 3
    timeout_seconds: 30
    temperature: 0.1
    max_tokens: 1000
    
    # Quality thresholds
    thresholds:
      selection_confidence: 0.5
      disambiguation_confidence: 0.5
      decomposition_confidence: 0.5
    
    # Logging configuration
    logging:
      log_decisions: true
      log_transformations: true
      log_timing: true

# Concept detection configuration  
concepts:
  # Vector index settings for concept candidates
  candidates:
    collection_name: "concept_candidates"
    similarity_threshold: 0.9  # For detecting existing concepts
    
  # Canonical concepts vector store
  canonical:
    collection_name: "concepts"
    similarity_threshold: 0.95  # For merging similar concepts

# Logging configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  format: "structured"  # "structured" or "simple"
  
  # Service identification for structured logs
  service_name: "clarifai"
  
  # Log to stdout/stderr following idea-logging.md
  handlers:
    - type: "console"
      stream: "stdout"

# Plugin configuration
plugins:
  # Plugin discovery and loading
  auto_discover: true
  plugin_directories:
    - "shared/clarifai_shared/plugins"
  
  # Default plugin settings
  default_plugin:
    enabled: true
    llm_fallback: true

# Scheduler configuration
scheduler:
  # Job configurations
  jobs:
    concept_embedding_refresh:
      enabled: true
      cron: "0 3 * * *"  # 3 AM daily
      description: "Refresh concept embeddings from Tier 3 pages"
    
    vault_sync:
      enabled: true
      cron: "*/30 * * * *"  # Every 30 minutes
      description: "Sync vault files with knowledge graph"

# Feature flags
features:
  # Sprint 2 features
  embedding_enabled: true
  chunking_enabled: true
  pgvector_enabled: true
  
  # Future features (disabled for MVP)
  evaluation_agents: false
  concept_linking: false
  tier2_generation: false

# Processing configuration
processing:
  # Batch sizes for various operations
  batch_sizes:
    embedding: 50      # Documents to embed at once
    chunking: 100      # Documents to chunk at once
    
  # Retry configuration following on-error-handling-and-resilience.md
  retries:
    max_attempts: 3
    backoff_factor: 2
    max_wait_time: 60

# Development and testing
development:
  debug: false
  test_mode: false
  mock_llm: false
  mock_embedding: false